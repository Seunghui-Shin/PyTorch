{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab10_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "이번 LAB은 퀴즈로만 구성되어 있습니다."
      ],
      "metadata": {
        "id": "4uSVZ2RzgJxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. 다음과 같이 conv1 의 이름으로 convolution layer 필터를 만들어 주고 inputs를 넣어주었습니다.\n",
        "\n",
        "conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "\n",
        "inputs = (A,B,C,D)\n",
        "\n",
        "output = conv1(inputs)\n",
        "\n",
        "위 코드에서 inputs의 형태로 알맞은 것을 서술하시오.\n",
        "(Height, Channel, Width, Batch size)\n",
        "\n",
        "A : \n",
        "\n",
        "B : \n",
        "\n",
        "C : \n",
        "\n",
        "D : "
      ],
      "metadata": {
        "id": "gF_maLR0gHMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Convolution의 output image size를 구하시오.\n",
        "\n",
        "input image size =  32 X 32\n",
        "\n",
        "filter size = 5 X 5\n",
        "\n",
        "stride = 1\n",
        "\n",
        "padding = 2\n",
        "\n",
        "=> output image size = ?"
      ],
      "metadata": {
        "id": "wzfv6owvhFLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Convolution의 output image size를 구하시오.\n",
        "\n",
        "input image size =  32 X 64\n",
        "\n",
        "filter size = 5 X 5\n",
        "\n",
        "stride = 1\n",
        "\n",
        "padding = 0\n",
        "\n",
        "=> output image size = ?"
      ],
      "metadata": {
        "id": "FrwFeiSYhaU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. 출력 결과는?\n",
        "\n",
        "inputs = torch.Tensor(1,1,32,32)\n",
        "\n",
        "conv1 = torch.nn.Conv2d(1,1,(5,5), stride = 1, padding = 2)\n",
        "\n",
        "output = conv1(inputs)\n",
        "\n",
        "print(output.size())\n",
        "\n",
        "=> torch.Size([ , , , ])"
      ],
      "metadata": {
        "id": "v91w87NcmNgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. 채널이 8인 42x42 input 이미지와 5x5의 16채널 필터를 \"stride=1\"로 convolution 연산을 하되, input과 같은 크기의 ouput 결과를 가져오도록 하려고 한다. 이 때, 얼마의 padding을 주어야 하는가?\n",
        "\n",
        "padding = ?"
      ],
      "metadata": {
        "id": "Ynqfr96OhbxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Convolution layer에 3X3 필터와 stride=1을 사용하여 더 적은 파라미터를 통해 학습의 효율성을 높인 CNN 모델은?\n",
        "\n",
        "=> "
      ],
      "metadata": {
        "id": "rDs0MKwgiYcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Residual block을 통한 skip connection으로 연산량을 줄이고 gradient vanishing, exploding 문제를 해결한 CNN 모델은?\n",
        "\n",
        "=> "
      ],
      "metadata": {
        "id": "QnSa8FHbjd27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. GoogleNet에서 활용되는 인셉션 모듈에서 (1,1) 사이즈의 필터를 사용하는 이유는?\n",
        "\n",
        "=> "
      ],
      "metadata": {
        "id": "AaSOqXQBj4v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. GoogleNet에서 사용하는 auxiliary classifier의 역할과 장점은?\n",
        "\n",
        "=> "
      ],
      "metadata": {
        "id": "GyfEJgjMkz0_"
      }
    }
  ]
}